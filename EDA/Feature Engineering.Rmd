---
title: "Feature Engineering"
author: "Jan Hynek, Štìpán Svoboda, Nursultan Svankulov"
date: "12 listopadu 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
During this analysis, as we have nice and big dataset, we decided to use text2vec package, created by Dmitri Selivanov.
Its main benefit is its efficiency, on the other hand its documentation is sometimes bit lacking. However, this will allow us to do word embeddings which we consider as a nice touch for our feature engineering. 
```{r packages}
library(tidyverse)
library(feather)
library(text2vec)
library(tokenizers)
library(stringr)
library(slam)
library(lubridate)
library(textstem)
```

Loading data. We have files in one folder and data in another folder, therefore we set working directory like this. Afterwards, we set share of data which we would like to work with during this analysis, as working with whole dataset is usually inconvinient and uneccessary. However, we calculated results for the whole
```{r loading data, echo = FALSE, warning=FALSE, error=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("..")

share_of_data <- 1
set.seed(926021)
d <- read_feather("data/ph_ads_payment_indicator.feather")
if (share_of_data <= 1){
    subdf <- d
    rm(d)
} else{
    subdf <- d %>% 
    mutate(rnd = runif(dim(subdf[1]))) %>%
    filter(rnd < share_of_data)
    subdf$rnd <- NULL
    rm(d)
}
```


# Document term matrix creation
In this section we prepare the features using text. 
We create our preparation function, which will be applied to all texts. This consists of making all letters lower case, removing numbers, one- and two-letter words and omitting whitespace.
Then we create tokenizer function. As we would like to create document - term matrix, we need word tokenizer.

```{r prep and token fun}
prep_fun <- function(x) {
  x <- tolower(x) # lower case
  x <- str_replace_all(x, '[:digit:]', ' ') # removing numbers
  x <- str_replace_all(x, '\\b\\w{1,2}\\b',' ') # removing one and two letter words
  x <- str_replace_all(x, '[:punct:]', ' ') # removing punctuation
  x <- str_replace_all(x, '\\s+', ' ')# removing white space
  x <- lemmatize_strings(x) # lemmatization of words. similar to stemming
  return(x)
}
tok_fun <- function(x){
  tokenize_words(x, stopwords = stopwords()) # removing stopwords
  } 
```

These functions are afterwards fed to iterator, which iterates on the descriptions. Then iterator creates vocabulary in a format of sparse matrix. In our case dataset consists of _432 thousand_ different terms. 

```{r iterator and vocab}
iterator <- itoken(subdf$description, # data to clean
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = subdf$id, 
                  progressbar = TRUE)
vocab <- create_vocabulary(iterator)
```
However, most of these words usually appears only once in the whole dataset. It is useful then to prune these words. We chose arbitrarily that word has to be at least 10 times in the dataset, in at least 0.1%  and at most 50% of the documents. This left us with ~2500 words. According to Paul Nation and Robert Waring (1997), this is equal to text coverage of ~80%. 
```{r pruning vocab}
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.0005)

```

Now, we are ready to vectorize the results and create Document - Term Matrix.
```{r dtm}
vectorizer <- vocab_vectorizer(pruned_vocab)
dtm <- create_dtm(iterator, vectorizer)
```

Let's have a look at the DTM. Dimensions fits - 4M rows, 2500 columns.
The least common word is 4000 times present in the dataset, the most common one have almost 750k occurences.
```{r dtm stats}
dim(dtm)
summary(col_sums(dtm))
```

What are the most common words? 
```{r most common}
sort(col_sums(dtm), decreasing = TRUE)[1:24]

```
What about the least present words? Do they make any sense?
```{r least common}
sort(col_sums(dtm), decreasing = FALSE)[1:24]
```

```{r wordcloud}
library(wordcloud)
freq <- data.frame(freqterms = sort(col_sums(dtm), decreasing = TRUE))
wordcloud(rownames(freq), freq[, 1], max.words=50, colors = brewer.pal(3, "Dark2"))
```
Let's do the same for the titles as well.
```{r dtm and wordcloud for titles}
iterator_title <- itoken(subdf$title, 
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = subdf$id, 
                  progressbar = TRUE)
vocab_title <- create_vocabulary(iterator_title)
pruned_vocab_title <- prune_vocabulary(vocab_title, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.0001)
vectorizer_title <- vocab_vectorizer(pruned_vocab_title)
dtm_title <- create_dtm(iterator_title, vectorizer_title)
freq <- data.frame(freqterms = sort(col_sums(dtm_title), decreasing = TRUE))
wordcloud(rownames(freq), freq[, 1], max.words=50, colors = brewer.pal(3, "Dark2"))
```
What about the dimensions?
```{r}
dim(dtm_title)
```
We have less words. This is expected, as titles have less words overall.


We also add alternative dtm based on tfidf. This will be later used for modelling and comparing, which of these approaches is better.
```{r}
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_tfidf = fit_transform(dtm, tfidf)
dtm_title_tfidf = fit_transform(dtm_title, tfidf)


```


#Metadata creation

We decided to harvest other important information from the text. We think it might be useful to know how many letters are in uppercase, how many punctuation is in the text and else.


```{r}
subdf <- subdf %>% mutate(upper_descr = str_count(description, "[:upper:]")) %>% 
  mutate(punct_descr = str_count(description, "[:punct:]")) %>% 
  mutate(length_descr = str_count(description)) %>% 
  mutate(excl_descr = (str_count(description, "!") > 0)) %>%
  mutate(has_capslock_descr = str_count(description, "[:upper:]") > 0) %>% 
  mutate(upper_title = str_count(title, "[:upper:]")) %>% 
  mutate(punct_title = str_count(title, "[:punct:]")) %>% 
  mutate(length_title = str_count(title)) %>% 
  mutate(excl_title = (str_count(title, "!") > 0)) %>%
  mutate(has_capslock_title = str_count(title, "[:upper:]") > 0) %>%
  mutate(n_words_title = str_count(title,'\\w+')) %>%
  mutate(n_words_descr = str_count(description,'\\w+'))

tok_fun_dplyr <- function(x){
  x <- tokenize_words(x, stopwords = stopwords())
  x <- unlist(x)
  return(x)
  } 


abc <- subdf %>% 
  select(description, n_words_descr) %>% 
  transmute(sum(hunspell_check(tok_fun_dplyr(description)))/n_words_descr)


```
We can also get rid of several variables. After thorough EDA we have found that these variables do not add value for future analysis.
```{r}
subdf$has_gg <- NULL
subdf$has_skype <- NULL
subdf$has_phone <- NULL
```


Now it is time to add DTMs to original table, and save!
```{r}
subdf2 <- cbind(subdf, dtm)
```

