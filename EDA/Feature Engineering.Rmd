---
title: "Feature Engineering"
author: "Jan Hynek, Štìpán Svoboda, Nursultan Svankulov"
date: "12 listopadu 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
During this analysis, as we have nice and big dataset, we decided to use text2vec package, created by Dmitri Selivanov.
Its main benefit is its efficiency, on the other hand its documentation is sometimes bit lacking. However, this will allow us to do word embeddings which we consider as a nice touch for our feature engineering. 
```{r packages}
library(tidyverse)
library(feather)
library(text2vec)
library(tokenizers)
library(stringr)
library(slam)
library(lubridate)
```

Loading data. We have files in one folder and data in another folder, therefore we set working directory like this. Afterwards, we set share of data which we would like to work with during this analysis, as working with whole dataset is usually inconvinient and uneccessary. However, we calculated results for the whole
```{r loading data, echo = FALSE, warning=FALSE, error=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("..")

share_of_data <- 0.01
set.seed(926021)
data <- read_feather("data/ph_ads_payment_indicator.feather")
subdf <- data %>% 
  mutate(rnd = runif(dim(subdf[1]))) %>%
  filter(rnd < share_of_data)
```


# Document term matrix creation
In this section we prepare the features using text. 
We create our preparation function, which will be applied to all texts. This consists of making all letters lower case, removing numbers, one- and two-letter words and omitting whitespace.
Then we create tokenizer function. As we would like to create document - term matrix, we need word tokenizer.

```{r prep and token fun}
prep_fun <- function(x) {
  x <- tolower(x) # lower case
  x <- str_replace_all(x, '[:digit:]', ' ') # removing numbers
  x <- str_replace_all(x, '\\b\\w{1,2}\\b',' ')  # removing one and two letter words
  x <- str_replace_all(x, '\\s+', ' ') # removing white space
}
tok_fun <- function(x){tokenize_words(x, stopwords = stopwords())} # removing stopwords
```

These functions are afterwards fed to iterator, which iterates on the descriptions. Then iterator creates vocabulary in a format of sparse matrix. In our case dataset consists of _534 thousand_ different terms. 

```{r iterator and vocab}
iterator <- itoken(subdf$description, # data to clean
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = subdf$id, 
                  progressbar = TRUE)
vocab <- create_vocabulary(iterator)
```
However, most of these words usually appears only once in the whole dataset. It is useful then to prune these words. We chose arbitrarily that word has to be at least 10 times in the dataset, in at least 0.1%  and at most 50% of the documents. This left us with ~2500 words. According to Paul Nation and Robert Waring (1997), this is equal to text coverage of ~80%. 
```{r pruning vocab}
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)

```
Now, we are ready to vectorize the results and create Document - Term Matrix.
```{r dtm}
vectorizer <- vocab_vectorizer(pruned_vocab)
dtm <- create_dtm(iterator, vectorizer)
```

Let's have a look at the DTM. Dimensions fits - 4M rows, 2500 columns.
The least common word is 4000 times present in the dataset, the most common one have almost 750k occurences.
```{r dtm stats}
dim(dtm)
summary(col_sums(dtm))
```

What are the most common words? 
```{r most common}
sort(col_sums(dtm), decreasing = TRUE)[1:24]

```
What about the least present words? Do they make any sense?
```{r least common}
sort(col_sums(dtm), decreasing = FALSE)[1:24]
```

```{r wordcloud}
library(wordcloud)
freq <- data.frame(freqterms = sort(col_sums(dtm), decreasing = TRUE))
wordcloud(rownames(freq), freq[, 1], max.words=50, colors = brewer.pal(3, "Dark2"))
```
Let's do the same for the titles as well.
```{r dtm and wordcloud for titles}
iterator_title <- itoken(subdf$title, 
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = subdf$id, 
                  progressbar = TRUE)
vocab_title <- create_vocabulary(iterator_title)
pruned_vocab_title <- prune_vocabulary(vocab_title, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.0001)
vectorizer_title <- vocab_vectorizer(pruned_vocab_title)
dtm_title <- create_dtm(iterator_title, vectorizer_title)
freq <- data.frame(freqterms = sort(col_sums(dtm_title), decreasing = TRUE))
wordcloud(rownames(freq), freq[, 1], max.words=50, colors = brewer.pal(3, "Dark2"))
```
What about the dimensions?
```{r}
dim(dtm_title)
```
We have less words. This is expected, as titles have less words overall.

#Metadata creation

We decided to harvest other important information from the text. We think it might be useful to know how many letters are in uppercase, how many punctuation is in the text and else.
-punctuation
-uppercase
-share of uppercae
-share of punctuation
-



```{r}
metadata_creator <- function(string, varname){
  result <- c()
  result[1] <- str_count(string, "[:upper:]")
  result[2] <- str_count(string, "[:punct:]")
  names(result) <- c("upper", 'punct')
  names(result) <- paste(names(result), varname, sep = "_")
}
```











# Word embeddings
We decided to use do word embeddings in our feature engineering. We will use them to create combined features, group of words with similar semantical meanings. We think that such features could have stronger effect in prediction.
```{r}
tcm <- create_tcm(iterator, vectorizer)
print(dim(tcm))
```
Global vectors recalculates space of words. They can also be used to reduce dimensionality of original TCM. We can lower the dimensions to just two (even though best practice is using at least 50 dimensions) and have a look, whether it is working in this case. Let's have a look on the top 100 most frequent words.


```{r}
glove_viz <- GlobalVectors$new(word_vectors_size = 2, vocabulary = pruned_vocab, x_max = 10)
vec_main_viz <- glove_viz$fit_transform(tcm, n_iter = 20)
wv_context_viz <- glove_viz$components
word_vectors_viz <- vec_main_viz + t(wv_context_viz)
d_viz <- dist2(word_vectors_viz, method = "cosine")  # smaller values means closer
wv_viz <- word_vectors_viz[(rownames(word_vectors_viz) %in% 
                          names(sort(col_sums(dtm), 
                                     decreasing = TRUE)[1:100])), ]

wv_viz <- as.tbl(as.data.frame(wv_viz))
ggplot(wv_viz, aes(V1, V2, label = rownames(wv_viz))) + 
  geom_text(check_overlap = FALSE) 

```

Overall, we can see several clusters. Top right corner have words which often are used with electronics. Bottom right on the other hand have words used with estate. we can see also see cluster of words in the middle, which can be used with both, such as "price" or "sale". Bottom middle is also locational cluster, which is more important for real estate, but this cluster stands littlebit out. 

Let's calculate proper vectors.

```{r}
glove <- GlobalVectors$new(word_vectors_size = 50, vocabulary = pruned_vocab, x_max = 10)
vectors_main <- glove$fit_transform(tcm, n_iter = 20)
word_vectors_context <- glove$components
word_vectors <- vectors_main + t(word_vectors_context)
d <- dist2(word_vectors, method = "cosine")  # smaller values means closer


# finding the similar words
find_close_words <- function(w, d, n) {
  words <- rownames(d)
  i <- which(words == w)
  if (length(i) > 0) {
    res <- sort(d[i,])
    print(as.matrix(res[2:(n + 1)]))
  } 
  else {
    print("Word not in corpus.")
  }
}
```


Let's have a look at some words: price, manila, iphone, bedroom
```{r}
find_close_words("price", d, 10)
find_close_words("manila", d, 10)
find_close_words("iphone", d, 10)
find_close_words("bedroom", d, 10)
```
We can observe that these words are similar to the same words as before in the case of two dimensions.

We will use clustering to create our new features. In the case of two dimensions, 5 clusters look like this.
```{r}
wv_viz2 <- wv_viz %>% mutate(cluster = factor(kmeans(wv_viz, 5)$cluster))
ggplot(wv_viz2, aes(V1, V2, label = rownames(wv_viz))) + 
  geom_text(check_overlap = FALSE, aes(color = cluster)) +
    scale_color_viridis(discrete=TRUE) +
    theme_bw() 

```
How many clusters for the whole dataset, so how many features will we create? We will left this question for later.
```{r}
n_clusters <- 50

word_vectors_cluster <- word_vectors %>% 
  as.data.frame() %>% 
  as.tbl() %>% mutate(cluster = factor(kmeans(. , n_clusters)$cluster))
dim(word_vectors_cluster)


word_vectors_cluster %>% group_by(cluster) %>% count() %>% ggplot(aes(n)) + geom_histogram(binwidth = 1)
```

We are quite lucky - only one group in kmeans have less than 20 observations, and only one group have more than 100 observations. This is not optimal, as we would like to have groups of same size, however the differences between groups are not so large overall.

```{r}
new_features <- as.data.frame(rep(NA, dim(dtm)[1]))
counter <- 1
for (i in sort(unique(word_vectors_cluster$cluster))){
  current_names <- rownames(word_vectors)[word_vectors_cluster$cluster == i]
  new_features[, counter] <- row_sums(dtm[, colnames(dtm) %in% current_names])
  counter <- counter + 1
  
}

```
```{r}
summary(row_sums(new_features))
summary(row_sums(dtm))
```
Hooray, it worked! We will se later throughout the analysis whether these features have any use. 
TODO:
- same feature extraction, but with titles
- Metadata extraction
- putting it all together:
    - dtm, dtm_title, new_features, new_features_title, metadata, metadata_title
- saving resulting matrix
    -it's gonna be yuuuuuuuuuuuuuuuge



