---
title: "GloVe - addendum"
author: "Jan Hynek, Štìpán Svoboda, Nursultan Svankulov"
date: "14 listopadu 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(tidyverse)
library(feather)
library(text2vec)
library(tokenizers)
library(stringr)
library(slam)
library(lubridate)
```

Loading data. We have files in one folder and data in another folder, therefore we set working directory like this. Afterwards, we set share of data which we would like to work with during this analysis, as working with whole dataset is usually inconvinient and uneccessary. However, we calculated results for the whole
```{r loading data, echo = FALSE, warning=FALSE, error=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("..")

share_of_data <- 0.01
set.seed(926021)
data <- read_feather("data/ph_ads_payment_indicator.feather")
subdf <- data %>% 
  mutate(rnd = runif(dim(subdf[1]))) %>%
  filter(rnd < share_of_data)
```


# Document term matrix creation
In this section we prepare the features using text. 
We create our preparation function, which will be applied to all texts. This consists of making all letters lower case, removing numbers, one- and two-letter words and omitting whitespace.
Then we create tokenizer function. As we would like to create document - term matrix, we need word tokenizer.

```{r prep and token fun}
prep_fun <- function(x) {
  x <- tolower(x) # lower case
  x <- str_replace_all(x, '[:digit:]', ' ') # removing numbers
  x <- str_replace_all(x, '\\b\\w{1,2}\\b',' ')  # removing one and two letter words
  x <- str_replace_all(x, '\\s+', ' ') # removing white space
}
tok_fun <- function(x){tokenize_words(x, stopwords = stopwords())} # removing stopwords
```

These functions are afterwards fed to iterator, which iterates on the descriptions. Then iterator creates vocabulary in a format of sparse matrix. In our case dataset consists of _534 thousand_ different terms. 

```{r iterator and vocab}
iterator <- itoken(subdf$description, # data to clean
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = subdf$id, 
                  progressbar = TRUE)
vocab <- create_vocabulary(iterator)
```

```{r pruning vocab}
pruned_vocab <- prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)

```
Now, we are ready to vectorize the results and create Document - Term Matrix.
```{r dtm}
vectorizer <- vocab_vectorizer(pruned_vocab)
```

# Word embeddings
We decided tohave a look at word embeddings in our feature engineering. We were thinking about using them to create combined features, group of words with similar semantical meanings. We think that such features could have stronger effect in prediction. However, at this moment this results only in colinear variables.
```{r}
tcm <- create_tcm(iterator, vectorizer)
print(dim(tcm))
```
Global vectors recalculates space of words. They can also be used to reduce dimensionality of original TCM. We can lower the dimensions to just two (even though best practice is using at least 50 dimensions) and have a look, whether it is working in this case. Let's have a look on the top 100 most frequent words.


```{r}
glove_viz <- GlobalVectors$new(word_vectors_size = 2, vocabulary = pruned_vocab, x_max = 10)
vec_main_viz <- glove_viz$fit_transform(tcm, n_iter = 20)
wv_context_viz <- glove_viz$components
word_vectors_viz <- vec_main_viz + t(wv_context_viz)
d_viz <- dist2(word_vectors_viz, method = "cosine")  # smaller values means closer
wv_viz <- word_vectors_viz[(rownames(word_vectors_viz) %in% 
                          names(sort(col_sums(dtm), 
                                     decreasing = TRUE)[1:100])), ]

wv_viz <- as.tbl(as.data.frame(wv_viz))
ggplot(wv_viz, aes(V1, V2, label = rownames(wv_viz))) + 
  geom_text(check_overlap = FALSE) 

```



Overall, we can see several clusters. Top right corner have words which often are used with electronics. Bottom right on the other hand have words used with estate. we can see also see cluster of words in the middle, which can be used with both, such as "price" or "sale". Bottom middle is also locational cluster, which is more important for real estate, but this cluster stands littlebit out. 

Let's calculate proper vectors.

```{r}
glove <- GlobalVectors$new(word_vectors_size = 50, vocabulary = pruned_vocab, x_max = 10)
vectors_main <- glove$fit_transform(tcm, n_iter = 20)
word_vectors_context <- glove$components
word_vectors <- vectors_main + t(word_vectors_context)
d <- dist2(word_vectors, method = "cosine")  # smaller values means closer


# finding the similar words
find_close_words <- function(w, d, n) {
  words <- rownames(d)
  i <- which(words == w)
  if (length(i) > 0) {
    res <- sort(d[i,])
    print(as.matrix(res[2:(n + 1)]))
  } 
  else {
    print("Word not in corpus.")
  }
}
```


Let's have a look at some words: price, manila, iphone, bedroom
```{r}
find_close_words("price", d, 10)
find_close_words("manila", d, 10)
find_close_words("iphone", d, 10)
find_close_words("bedroom", d, 10)
```
We can observe that these words are similar to the same words as before in the case of two dimensions.

We will use clustering to create our new features. In the case of two dimensions, 5 clusters look like this.
```{r}
wv_viz2 <- wv_viz %>% mutate(cluster = factor(kmeans(wv_viz, 5)$cluster))
ggplot(wv_viz2, aes(V1, V2, label = rownames(wv_viz))) + 
  geom_text(check_overlap = FALSE, aes(color = cluster)) +
    scale_color_viridis(discrete=TRUE) +
    theme_bw() 

```



How many clusters for the whole dataset, so how many features will we create? We will left this question for later.
```{r}
n_clusters <- 50

word_vectors_cluster <- word_vectors %>% 
  as.data.frame() %>% 
  as.tbl() %>% mutate(cluster = factor(kmeans(. , n_clusters)$cluster))
dim(word_vectors_cluster)


word_vectors_cluster %>% group_by(cluster) %>% count() %>% ggplot(aes(n)) + geom_histogram(binwidth = 1)
```



We are quite lucky - only one group in kmeans have less than 20 observations, and only one group have more than 100 observations. This is not optimal, as we would like to have groups of same size, however the differences between groups are not so large overall.

```{r}
new_features <- as.data.frame(rep(NA, dim(dtm)[1]))
counter <- 1
for (i in sort(unique(word_vectors_cluster$cluster))){
  current_names <- rownames(word_vectors)[word_vectors_cluster$cluster == i]
  new_features[, counter] <- row_sums(dtm[, colnames(dtm) %in% current_names])
  counter <- counter + 1
  
}

```
```{r}
summary(row_sums(new_features))
summary(row_sums(dtm))
```
Hooray, it worked! We will se later throughout the analysis whether these features have any use. 
TODO:
- same feature extraction, but with titles
- Metadata extraction
- putting it all together:
    - dtm, dtm_title, new_features, new_features_title, metadata, metadata_title
- saving resulting matrix
    -it's gonna be yuuuuuuuuuuuuuuuge
